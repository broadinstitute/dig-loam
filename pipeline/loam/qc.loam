import input._
import files._
import binaries._
import scripts._
import loamstream.loam.LoamStore
import store_helpers._



/** Alignment Step
 *  Description: Align data strand to 1KG reference. Also, update reference allele and variant ID to match 1KG
 *  Requires: Plink1.9 and, at least, Genotype Harmonizer v1.4.18
 *  Input: VCF file, 1000 Genomes reference VCF file
 *  Output Needed: ${outLABEL}.chr${CHROMOSOME}.bed/bim/fam, ${outLABEL}.chr${CHROMOSOME}.harmonized.bed/bim/fam/log(/nosex?/hh?), merge.txt, force_a2.txt, 
 *     ${outLABEL}.harmonized.sample, ${outLABEL}.chr${CHROMOSOME}.harmonized_idUpdates.txt, ${outLABEL}.chr${CHROMOSOME}.harmonized_snpLog.log
 *  Notes:
 *     Could also add --variants and --mafAlign as pipeline options, but for now these are static
 *     Ideally, this will be run in parallel by chromosome number
 */

val PLINK_CHRS: Array[Seq[LoamStore[TXT]]] = Array.ofDim(22)
val PLINK_CHRS_LOGS: Array[Seq[LoamStore[TXT]]] = Array.ofDim(22)
val PLINK_CHRS_HARMONIZED: Array[Seq[LoamStore[TXT]]] = Array.ofDim(22)
val PLINK_HARMONIZED = bedBimFam(outDIR / s"${outLABEL}.harmonized")
val VCF_BASE = outDIR / s"${outLABEL}.harmonized.ref"
val VCF = store[VCF](outDIR / s"${outLABEL}.harmonized.ref.vcf.gz")
val KG_VCFS: Array[LoamStore[VCF]] = Array.ofDim(22)
val IDUPDATES: Array[LoamStore[TXT]] = Array.ofDim(22)
val SNPLOGS: Array[LoamStore[TXT]] = Array.ofDim(22)
val MERGE_LIST = store[TXT].at(outDIR / "merge.txt")
val FORCE_A2 = store[TXT].at(outDIR / "force_a2.txt")
val SAMPLE_FILE = store[TXT].at(outDIR / s"${outLABEL}.harmonized.sample")

for(i <- 1 until 22) {
	PLINK_CHRS :+ bedBimFam(outDIR / s"${outLABEL}.chr${i}")
	PLINK_CHRS_HARMONIZED :+ bedBimFam(outDIR / s"${outLABEL}.chr${i}.harmonized")
	KG_VCFS :+ store[VCF].at($KG_VCF_BASE.replaceAll("\[CHROMOSOME\]",$i) + ".vcf.gz").asInput
	IDUPDATES :+ store[TXT].at(outDIR / s"${outLABEL}.chr${i}.harmonized_idUpdates.txt")
	SNPLOGS :+ store[TXT].at(outDIR / s"${outLABEL}.chr${i}.harmonized_snpLog.log")

	cmd"""$PLINK --bfile $inVCF --chr $i --keep-allele-order --make-bed --out $PLINK_CHRS(i-1)""".in(inVCF).out(PLINK_CHRS(i-1))

	cmd"""$GENOTYPE_HARMONIZER 
		--input $PLINK_CHRS(i-1)
		--inputType PLINK_BED
		--output $PLINK_CHRS_HARMONIZED(i-1)
		--outputType PLINK_BED
		--ref $KG_VCF_BASE.replaceAll("\[CHROMOSOME\]",$i)
		--refType VCF
		--keep
		--update-id
		--variants 1000
		--mafAlign 0.1
		--update-id
		--update-reference-allele
		--debug""".in(PLINK_CHRS(i-1),KG_VCFS(i-1)).out(PLINK_CHRS_HARMONIZED(i-1),IDUPDATES(i-1),SNPLOGS(i-1))
}

cmd"""echo "${PLINK_CHRS_HARMONIZED(1)}.bed ${PLINK_CHRS_HARMONIZED(1)}.bim ${PLINK_CHRS_HARMONIZED(1)}.fam" > $MERGE_LIST""".out(MERGE_LIST) 
for(i <- 3 until 22) {
	cmd"""echo "${PLINK_CHRS_HARMONIZED(i-1)}.bed ${PLINK_CHRS_HARMONIZED(i-1)}.bim ${PLINK_CHRS_HARMONIZED(i-1)}.fam" >> $MERGE_LIST""".in(MERGE_LIST).out(MERGE_LIST)
}

cmd"""$PLINK --bfile $PLINK_CHRS_HARMONIZED(0) --merge-list $MERGE_LIST --make-bed --keep-allele-order --out $PLINK_HARMONIZED""".in(PLINK_CHRS_HARMONIZED, MERGE_LIST).out(PLINK_HARMONIZED)

cmd"""awk '{print $$2,$$5}' ${PLINK_HARMONIZED}.bim > $FORCE_A2""".in(PLINK_HARMONIZED).out(FORCE_A2)

cmd"""$PLINK --bfile $PLINK_HARMONIZED --recode vcf-iid bgz --real-ref-alleles --a2-allele $FORCE_A2 --out $VCF_BASE""".in(PLINK_HARMONIZED, FORCE_A2).out(VCF)

cmd"""$TABIX -p vcf $VCF""".in(VCF)

cmd"""echo "IID POP SUPERPOP SEX" > $SAMPLE_FILE""".out(SAMPLE_FILE)

cmd"""awk -v v=${outLABEL} '{if($$5 == 1) { sex="male" } else { if($$5 == 2) { sex="female" } else { sex="NA" } } print $$2" "v" "v" "sex}' ${PLINK_HARMONIZED}.fam >> $SAMPLE_FILE""".in(PLINK_HARMONIZED, SAMPLE_FILE).out(SAMPLE_FILE)



/** Load Step
 *  Description: Generate the Hail VDS from VCF file and a sample file containing population and sex information
 *  Requires: Hail, Java (version under which Hail was compiled)
 *  Input: $VCF, $SAMPLE_FILE
 *  Output Needed: ${outLABEL}.harmonized.ref.vds/, ${outLABEL}.harmonized.ref.vds.log
 *  Notes: 
 *     Monomorphic variants are automatically removed during import into Hail
 */

val VDS = path(outDIR / s"${outLABEL}.harmonized.ref.vds")
val LOAD_LOG = store[TXT].at(outDIR / s"${outLABEL}.harmonized.ref.vds.log")

cmd"""$HAIL -l $LOAD_LOG
	importvcf --force-bgz $VCF
	splitmulti
	deduplicate
	annotatesamples table
	--root sa.pheno
	-e IID
	-i $SAMPLE_FILE
	-t "IID: String, POP: String, SUPERPOP: String, SEX: String"
	--missing "NA"
	--delimiter " "
	write
	-o $VDS
	count
	-g""".in(VCF,SAMPLE_FILE).out(LOAD_LOG, VDS)



/** Filter Step
 *  Description: Generate filtered and filtered/pruned filesets for QC
 *  Requires: Hail, Plink, Java (version under which Hail was compiled)
 *  Input: $VDS, $inREGIONS_EXCLUDE
 *  Output: ${outLABEL}.filter.log, ${outLABEL}.variantqc.tsv, ${outLABEL}.for_qc.vds, ${outLABEL}.for_qc.pruned.vds, ${outLABEL}.for_qc.bed/bim/fam, ${outLABEL}.for_qc.prune.in, ${outLABEL}.for_qc.prune.out
 *  Notes: 
 */

val VDS_FOR_QC = path(outDIR / s"${outLABEL}.for_qc.vds")
val VDS_FOR_QC_PRUNED = path(outDIR / s"${outLABEL}.for_qc.pruned.vds")
val PLINK_FOR_QC: bedBimFam(outDIR / s"${outLABEL}.for_qc")
val PLINK_FOR_QC_PRUNED: bedBimFam(outDIR / s"${outLABEL}.for_qc.pruned")
val FILTER_LOG = store[TXT].at(outDIR / s"${outLABEL}.filter.log")
val FILTERPRUNED_LOG = store[TXT].at(outDIR / s"${outLABEL}.filter.pruned.log")
val VARIANTQC_TSV = store[TXT].at(outDIR / s"${outLABEL}.variantqc.tsv")
val FOR_QC_PRUNE_IN = store[TXT].at(outDIR / s"${outLABEL}.for_qc.prune.in")
val FOR_QC_PRUNE_OUT = store[TXT].at(outDIR / s"${outLABEL}.for_qc.prune.out")

cmd"""$HAIL -l $FILTER_LOG
	read -i $VDS
	variantqc
	exportvariants -c "ID = v, Chrom = v.contig, Pos = v.start, Ref = v.ref, Alt = v.alt, va.qc.*"
	-o $VARIANTQC_TSV
	filtervariants expr -c 'v.altAllele.isSNP && ! v.altAllele.isComplex && v.isAutosomal && ["A","C","G","T"].toSet.contains(v.altAllele.ref) && ["A","C","G","T"].toSet.contains(v.altAllele.alt) && va.qc.AF >= 0.01 && va.qc.callRate >= 0.98' --keep
	filtervariants intervals -i $inREGIONS_EXCLUDE --remove
	write
	-o $VDS_FOR_QC
	exportplink
	-o $PLINK_FOR_QC""".in(VDS,inREGIONS_EXCLUDE).out(FILTER_LOG,VDS_FOR_QC,PLINK_FOR_QC)
	
cmd"""$PLINK --bfile $PLINK_FOR_QC --indep-pairwise 1500 150 0.2 --out $PLINK_FOR_QC""".in(PLINK_FOR_QC).out(FOR_QC_PRUNE_IN,FOR_QC_PRUNE_OUT)

cmd"""$HAIL -l $FILTERPRUNED_LOG
	read -i $VDS_FOR_QC
	filtervariants list -i $FOR_QC_PRUNE_IN --keep
	write
	-o $VDS_FOR_QC_PRUNED
	exportplink
	-o $PLINK_FOR_QC_PRUNED""".in(VDS_FOR_QC, FOR_QC_PRUNE_IN).out(FILTERPRUNED_LOG, VDS_FOR_QC_PRUNED, PLINK_FOR_QC_PRUNED)


/** Kinship Step
 *  Description: Calculate kinship to identify duplicates and any samples exhibiting abnormal (excessive) sharing
 *  Requires: King, R, $CALC_KINSHIP_SAMPLE_SHARING_R
 *  Input: $PLINK_FOR_QC
 *  Output: ${outLABEL}.kinshipTMP.dat, ${outLABEL}.kinshipTMP.ped, ${outLABEL}.kinship.kin, ${outLABEL}.kinship.kin0, ${outLABEL}.kinship.kin0.related, ${outLABEL}.kinship.sharing_counts.txt
 *  Notes: 
 *     King is preferred to Plink or Hail based IBD calcs due to robust algorithm handling of population stratification. This step should be followed by a visual inspection for duplicates or excessive sharing
       King only writes the '.kin0' file if families are found, so there needs to be a way to skip the second and third command if it doesn't get created
 */

val KIN_PREFIX = outDIR / s"${outLABEL}.kinship"
val KIN_LOG = store[TXT].at(KIN_PREFIX + ".log")
val KINTMP_DAT = store[TXT].at(KIN_PREFIX + "TMP.dat")
val KINTMP_PED = store[TXT].at(KIN_PREFIX + "TMP.ped")
val KIN = store[TXT].at(KIN_PREFIX + ".kin")
val KIN0 = store[TXT].at(KIN_PREFIX + ".kin0")
val KIN0_RELATED = store[TXT].at(KIN_PREFIX + ".kin0.related")
val SHARING_COUNTS = store[TXT].at(KIN_PREFIX + ".sharing_counts.txt")
	
cmd"""$KING -b ${PLINK_FOR_QC_PRUNED}.bed --kinship --prefix $KIN_PREFIX > $KIN_LOG""".in(PLINK_FOR_QC_PRUNED).out(KIN_LOG,KIN,KIN0,KINTMP_DAT,KINTMP_PED)

cmd"""(head -1 $KIN0; sed '1d' $KIN0 | awk '{if($$8 >= 0.0884) print $$0}' | sort -rn -k8,8) > $KIN0_RELATED""".in(KIN0).out(KIN0_RELATED)

cmd"""$R --vanilla --args $KIN0_RELATED $SHARING_COUNTS < $CALC_KINSHIP_SAMPLE_SHARING_R""".in(KIN0_RELATED, CALC_KINSHIP_SAMPLE_SHARING_R).out(SHARING_COUNTS)



/** Ancestry PCA Step
 *  Description: Calculate PCs combined with 1KG Phase 3 Purcell 5k data
 *  Requires: Hail, R, $PLOT_ANCESTRY_PCA_R
 *  Input: $VDS, $KG_HAIL, $KG_V3_5K_AF
 *  Output: ${outLABEL}.ancestry.pca.log, ${outLABEL}.ancestry.pca.scores.tsv, ${outLABEL}.ancestry.pca.loadings.tsv, .${outLABEL}.ancestry.pca.scores.tsv.crc, 
 *     ${outLABEL}.ancestry.pca.loadings.tsv.crc ${outLABEL}.ancestry.pca.scores.plots.pdf
 *  Notes: 
 *     To perform ancestry inference and clustering with 1KG data, we must combine on common variants with reference data (clustering does not work when only using PCA loadings and projecting)
 */

val ANCESTRYPCA_LOG = store[TXT].at(outDIR / s"${outLABEL}.ancestry.pca.log")
val ANCESTRYPCA_SCORES_TSV = store[TXT].at(outDIR / s"${outLABEL}.ancestry.pca.scores.tsv")
val ANCESTRYPCA_LOADINGS_TSV = store[TXT].at(outDIR / s"${outLABEL}.ancestry.pca.loadings.tsv")
val ANCESTRYPCA_SCORES_PLOTS_PDF = store[TXT].at(outDIR / s"${outLABEL}.ancestry.pca.scores.plots.pdf")

$HAIL -l $ANCESTRYPCA_LOG
	read $KG_HAIL
	put -n KG
	read -i $VDS
	join --right KG
	annotatevariants table $KG_V3_5K_AF
	-e Variant
	-c 'va.refPanelAF = table.refPanelAF'
	--impute
	pca -k 10
	--scores sa.pca.scores
	--eigenvalues global.pca.evals
	--loadings va.pca.loadings
	exportsamples -c 'IID = sa.pheno.IID, POP = sa.pheno.POP, SUPERPOP = sa.pheno.SUPERPOP, SEX = sa.pheno.SEX, PC1 = sa.pca.scores.PC1, PC2 = sa.pca.scores.PC2, PC3 = sa.pca.scores.PC3, PC4 = sa.pca.scores.PC4, PC5 = sa.pca.scores.PC5, PC6 = sa.pca.scores.PC6, PC7 = sa.pca.scores.PC7, PC8 = sa.pca.scores.PC8, PC9 = sa.pca.scores.PC9, PC10 = sa.pca.scores.PC10'
	-o $ANCESTRYPCA_SCORES_TSV
	exportvariants -c 'ID = v, PC1 = va.pca.loadings.PC1, PC2 = va.pca.loadings.PC2, PC3 = va.pca.loadings.PC3, PC4 = va.pca.loadings.PC4, PC5 = va.pca.loadings.PC5, PC6 = va.pca.loadings.PC6, PC7 = va.pca.loadings.PC7, PC8 = va.pca.loadings.PC8, PC9 = va.pca.loadings.PC9, PC10 = va.pca.loadings.PC10'
	-o $ANCESTRYPCA_LOADINGS_TSV""".in(KG_HAIL, VDS, KG_V3_5K_AF, ).out(ANCESTRYPCA_LOG, ANCESTRYPCA_SCORES_TSV, ANCESTRYPCA_LOADINGS_TSV)

cmd"""$R --vanilla --args $ANCESTRYPCA_SCORES_TSV $ANCESTRYPCA_SCORES_PLOTS_PDF < $PLOT_ANCESTRY_PCA_R""".in(ANCESTRYPCA_SCORES_TSV, PLOT_ANCESTRY_PCA_R).out(ANCESTRYPCA_SCORES_PLOTS_PDF)



/** Ancestry Cluster Step
 *  Description: Cluster with 1KG samples using Gaussian Mixture Modeling and infer ancestry
 *  Requires: Hail, R, $PLOT_ANCESTRY_CLUSTER_R, $outLABEL, $PHENO_ID, $PHENO_SR_RACE
 *  Input: ${outLABEL}.ancestry.pca.samples.scores.tsv, $PHENO
 *  Output: ${outLABEL}.ancestry.fet.1, ${outLABEL}.ancestry.temp.clu.1, ${outLABEL}.ancestry.clu.1, ${outLABEL}.ancestry.klg.1, ${outLABEL}.ancestry.cluster_plots.pdf, 
 *     ${outLABEL}.ancestry.cluster_xtabs, ${outLABEL}.ancestry.cluster_plots.centers.pdf, ${outLABEL}.ancestry.clusters_assigned, ${outLABEL}.ancestry
 *  Notes: 
 *     ${outLABEL}.ancestry contains the final inferred ancestry for each sample, including OUTLIERS
 *     This file may be updated after reconciling with other arrays
 */

val ANCESTRY_PREFIX = outDIR / s"${outLABEL}.ancestry"
val ANCESTRYCLUSTER_LOG = store[TXT].at(ANCESTRY_PREFIX + ".cluster.log")
val ANCESTRY_FET = store[TXT].at(ANCESTRY_PREFIX + ".fet.1")
val ANCESTRY_TEMP_CLU = store[TXT].at(ANCESTRY_PREFIX + ".temp.clu.1")
val ANCESTRY_CLU = store[TXT].at(ANCESTRY_PREFIX + ".clu.1")
val ANCESTRY_KLG = store[TXT].at(ANCESTRY_PREFIX + ".klg.1")
val ANCESTRY_CLUSTER_PLOTS_PDF = store[TXT].at(ANCESTRY_PREFIX + ".cluster_plots.pdf")
val ANCESTRY_CLUSTER_PLOTS_CENTERS_PDF = store[TXT].at(ANCESTRY_PREFIX + ".cluster_plots.centers.pdf")
val ANCESTRY_CLUSTER_PLOTS_NO1KG_PDF = store[TXT].at(ANCESTRY_PREFIX + ".cluster_plots.no_1kg.pdf")
val ANCESTRY_CLUSTER_XTABS = store[TXT].at(ANCESTRY_PREFIX + ".cluster_xtabs")
val ANCESTRY_CLUSTERS_ASSIGNED = store[TXT].at(ANCESTRY_PREFIX + ".clusters_assigned")
val ANCESTRY = store[TXT].at(ANCESTRY_PREFIX)

cmd"""echo 10 > $ANCESTRY_FET""".out(ANCESTRY_FET)

cmd"""sed '1d' $ANCESTRYPCA_SCORES_TSV | cut -f5- | sed 's/\t/ /g' >> $ANCESTRY_FET""".in(ANCESTRYPCA_SCORES_TSV).out(ANCESTRY_FET)

cmd"""$KLUSTAKWIK $ANCESTRY_PREFIX 1 -UseFeatures 1110000000 -UseDistributional 0 > $ANCESTRYCLUSTER_LOG""".in(ANCESTRY_FET).out(ANCESTRY_TEMP_CLU, ANCESTRY_CLU, ANCESTRY_KLG)

cmd"""$R --vanilla --args $ANCESTRYPCA_SCORES_TSV $ANCESTRY_CLU $PHENO $outLABEL $PHENO_ID $PHENO_SR_RACE 
	$ANCESTRY_CLUSTER_PLOTS_PDF $ANCESTRY_CLUSTER_XTABS $ANCESTRY_CLUSTER_PLOTS_CENTERS_PDF
	$ANCESTRY_CLUSTERS_ASSIGNED $ANCESTRY $ANCESTRY_CLUSTER_PLOTS_NO1KG_PDF 
	< $PLOT_ANCESTRY_CLUSTER_R""".in(ANCESTRYPCA_SCORES_TSV, ANCESTRY_CLU, PHENO, PLOT_ANCESTRY_CLUSTER_R).out(ANCESTRY_CLUSTER_PLOTS_PDF, ANCESTRY_CLUSTER_XTABS, ANCESTRY_CLUSTER_PLOTS_CENTERS_PDF, ANCESTRY_CLUSTERS_ASSIGNED, ANCESTRY, ANCESTRY_CLUSTER_PLOTS_NO1KG_PDF)



/** Non-Outlier PCA Step
 *  Description: Calculate PCs for all non-outlier samples combined (to be used for adjustment during sample outlier removal)
 *  Requires: Hail
 *  Input: $VDS_FOR_QC_PRUNED, $ANCESTRY
 *  Output: ${outLABEL}.pca.log, ${outLABEL}.pca.scores.tsv, ${outLABEL}.pca.loadings.tsv, .${outLABEL}.pca.scores.tsv.crc, 
 *     ${outLABEL}.pca.loadings.tsv.crc
 *  Notes: 
 */

val NONOUTLIERPCA_LOG = store[TXT].at(outDIR / s"${outLABEL}.pca.log")
val NONOUTLIERPCA_SCORES_TSV = store[TXT].at(outDIR / s"${outLABEL}.pca.scores.tsv")
val NONOUTLIERPCA_LOADINGS_TSV = store[TXT].at(outDIR / s"${outLABEL}.pca.loadings.tsv")

cmd"""$HAIL -l $NONOUTLIERPCA_LOG
	read $VDS_FOR_QC_PRUNED
	annotatesamples table
	-i $ANCESTRY
	--no-header
	-e _0
	--code "sa.pheno.IID = table._0, sa.pheno.POP = table._1, sa.pheno.SUPERPOP = table._1"
	filtersamples expr -c 'sa.pheno.SUPERPOP != "OUTLIERS"' --keep
	pca -k 10
	--scores sa.pca.scores
	--eigenvalues global.pca.evals
	--loadings va.pca.loadings
	exportsamples -c 'IID = sa.pheno.IID, POP = sa.pheno.POP, SUPERPOP = sa.pheno.SUPERPOP, SEX = sa.pheno.SEX, PC1 = sa.pca.scores.PC1, PC2 = sa.pca.scores.PC2, PC3 = sa.pca.scores.PC3, PC4 = sa.pca.scores.PC4, PC5 = sa.pca.scores.PC5, PC6 = sa.pca.scores.PC6, PC7 = sa.pca.scores.PC7, PC8 = sa.pca.scores.PC8, PC9 = sa.pca.scores.PC9, PC10 = sa.pca.scores.PC10'
	-o $NONOUTLIERPCA_SCORES_TSV
	exportvariants -c 'ID = v, PC1 = va.pca.loadings.PC1, PC2 = va.pca.loadings.PC2, PC3 = va.pca.loadings.PC3, PC4 = va.pca.loadings.PC4, PC5 = va.pca.loadings.PC5, PC6 = va.pca.loadings.PC6, PC7 = va.pca.loadings.PC7, PC8 = va.pca.loadings.PC8, PC9 = va.pca.loadings.PC9, PC10 = va.pca.loadings.PC10'
	-o $NONOUTLIERPCA_LOADINGS_TSV""".in(VDS_FOR_QC_PRUNED, ANCESTRY).out(NONOUTLIERPCA_SCORES_TSV, NONOUTLIERPCA_LOADINGS_TSV)


