loamstream {

  // job execution parameters
  // maxRunsPerJob [integer]: number of times a job is run before failing permanently
  // maxWaitTimeForOutputs [duration]: a Scala duration library compatible maximum time to wait for outputs from each job
  execution {

    maxRunsPerJob = 1
    maxWaitTimeForOutputs = "10 minutes"

    // singularity job parameters
    // mappedDirs [list[string]]: list of directories to map into (ie. make accessible from within) singularity containers
    // extraParams [string]: extra parameters to add to each singularity command
    singularity {
      mappedDirs = ["/path/to/dir"]
      extraParams = "--cleanenv"
    }

  }

  // google cloud settings
  // gcloudBinary [string]: path to gcloud executable
  // gsutilBinary [string]: path to gsutil executable
  // projectId [string]: existing google cloud project id
  // clusterId [string]: a name for the google cloud cluster created at runtime
  // credentialsFile [string]: google cloud credentials json file
  // region [string]: google cloud dataproc region name
  googlecloud {

    gcloudBinary = "/path/to/gcloud"
    gsutilBinary = "/path/to/gsutil"
    projectId = "my-google-cloud-project"
    clusterId = "mycluster"
    credentialsFile = "/path/to/google_credential.json"
    region = "us-central1"

    // default google cloud dataproc cluster configuration
    // zone [string]: google cloud dataproc zone name
    // properties [string]: spark cluster properties
    // masterMachineType [string]: master machine type
    // masterBootDiskSize [integer]: master boot disk size in gb
    // workerMachineType [string]: worker machine type
    // workerBootDiskSize [integer]: worker boot disk size in gb
    // numWorkers [integer]: number of worker machines
    // numPreemptibleWorkers [integer]: number of preemtible worker machines
    // preemptibleWorkerBootDiskSize [integer]: preemtible worker boot disk size in gb
    defaultClusterConfig {
      zone = "us-central1-a"
      properties = "spark:spark.speculation=true,spark:spark.driver.extraJavaOptions=-Xss4M,spark:spark.executor.extraJavaOptions=-Xss4M,spark:spark.driver.memory=45g,spark:spark.driver.maxResultSize=0,spark:spark.task.maxFailures=20,spark:spark.kryoserializer.buffer.max=1g,hdfs:dfs.replication=1"
      masterMachineType = "n1-highmem-2"
      masterBootDiskSize = 500
      workerMachineType = "n1-standard-2"
      workerBootDiskSize = 30
      numWorkers = 2
      numPreemptibleWorkers = 0
      preemptibleWorkerBootDiskSize = 30
    }

    // hail conda environment
    // condaEnv [string]: name of hail conda environment to duplicate in google cloud hail jobs
    hail {
      condaEnv = hail-0.2.95-loamstream
    }

  }

  // uger settings
  // maxNumJobs [integer]: maximum number of jobs to run at a time
  // defaultCores [integer]: default number of cores
  // defaultMemoryPerCore [integer]: default memory allocated per core in gb
  // staticJobSubmissionParams [string]: qsub job submission parameters
  uger {

    maxNumJobs = 2400
    defaultCores = 1
    defaultMemoryPerCore = 3
    staticJobSubmissionParams = "-cwd"

  }

}
