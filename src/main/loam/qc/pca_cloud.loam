import camp_cloud._
import binaries_cloud._

val m_base = store[TXT].from(base)
val m_phenotypes = store[TXT].from(phenotypes)
val m_interval_list = store[TXT].from(interval_list)
val m_vds = store[TXT].to(vds)
val m_pca_tsv = store[TXT].to(pca_tsv)

val m_cluster = "minimal"

cmd"""gcloud dataproc jobs submit spark
        --cluster $m_cluster
        --jar $hail_jar
        --class org.broadinstitute.hail.driver.Main
            importplink
            --bfile $m_base
            splitmulti
            annotatesamples table
            --root sa.pheno
            -e IID
            -i $m_phenotypes
            -t "IID: String, T2D_HEALTH_PROVIDER: Int, AGE_T2D_HEALTH_PROVIDER: Double, SEX: Int, SuperPopulation: String, RACE: String"
            --missing "NA"
            write
            --overwrite
            -o $m_vds
            count
            -g"""

cmd"""gcloud dataproc jobs submit spark
        --cluster $m_cluster
        --jar $hail_jar
        --class org.broadinstitute.hail.driver.Main
            read -i $m_vds
            filtervariants intervals
            --keep -i $m_interval_list
            pca --scores sa.pca
            exportsamples -c 'Sample = s, SuperPopulation = sa.pheno.SuperPopulation, Population = sa.pheno.RACE, PC1 = sa.pca.PC1, PC2 = sa.pca.PC2, PC3 = sa.pca.PC3, PC4 = sa.pca.PC4, PC5 = sa.pca.PC5, PC6 = sa.pca.PC6, PC7 = sa.pca.PC7, PC8 = sa.pca.PC8, PC9 = sa.pca.PC9, PC10 = sa.pca.PC10'
            -o $m_pca_tsv"""

